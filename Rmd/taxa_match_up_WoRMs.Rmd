---
title: "Taxonomix Matchup to WoRMs"
author: "Sebastian DiGeronimo"
date: "3/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("readxl")
library("hablar") # might be useful when needing to fix names
library("worrms")

library("tidyverse")
library("ggplot2")
library("ggforce")
library("tibble")
library("tidyr")
library("readr")
library("purrr")
library("dplyr")
library("stringr")
library("forcats")
library("lubridate")
library("glue")
library("fs")
library("geosphere")
library("vroom")
library("plotly")
library("magrittr")

```

```{r create-dir}
# will create a set directory if does not exists
# useful for new projects
source(here::here("scripts","create_dir.R"))

source(here::here("scripts", "taxa_list_check.R"))
```

```{r load-taxa-data}
# ---- load data set ----
file.taxa <- 
    fs::dir_ls(path =  here::here("data", "raw"),
           regexp = "^[^~]*\\.xlsx$") 
# %>%
    # last_mod(.)

# # remove later
# file.taxa <- fs::dir_ls(path =  here::here("data"),
#            regexp = "^[^~]*Analysis.*\\.xlsx$")  %>%
#     last_mod(.)

# find the number of skips to start of taxa information
skips <-  which(readxl::read_xlsx(file.taxa, range = cell_cols("A"), 
                                  col_names = FALSE ) == "Taxa") - 1

# extract values for calculating individuals per cubic meter
calc <-
    readxl::read_xlsx(
                      file.taxa[1], 
                      n_max = skips, 
                      col_names = c("x1", "x2", "x3")
                      ) %>% 
    select(-x3) %>%
    pivot_wider(#id_col      = x1,
                names_from  = x1,
                values_from = x2
                ) %>%
    janitor::clean_names() %>%
    mutate(
        date_collected = as.Date(as.numeric(date_collected), origin = "1899-12-30"),
        date_analyzed  = as.Date(as.numeric(date_analyzed), origin = "1899-12-30")
    ) %>%
    hablar::retype()

# load the data set
taxa <- readxl::read_xlsx(file.taxa,  
                          skip = skips, 
                          .name_repair = janitor::make_clean_names) %>%
                          mutate(
                              cruise_id = calc$sample_id,
                              .before = everything()
                          ) %>%
                        full_join(x = calc, by = c("sample_id" = "cruise_id")) %>%
                        select(cruise_id = sample_id, everything()) %>%
                        filter(mean > 0)

# rm(skips, file.taxa)
```

1. read in new occurrence file
2. read the first 10 rows and transpose to longer
3. determine if the identified species are in the master aphia taxa list, or 
    if need to rerun taxa_match_fix
4. row bind occurrence data with taxa info

```{r}
merge_taxa(taxa) 
    .Last.value %>%
    relocate(cruise_id, date_collected, site, .before = 1)
```

```{r}
source(here::here("scripts", "match_taxa_fix.R"))

taxa_file <-  
    fs::dir_ls(path =  here::here("data", "metadata"),
           regexp = "aphia_taxa.*\\.csv$")  %>%
    last_mod(.)
regex_lifestage <- paste0("copepodite|nauplii|larvae|larva|juvenile|",
                          "eggs|egg|zoea|protozoea|cypris|megalopa")
# ---- initialize taxa file ----
# if doesn't exists, will use first file as input
# contains original name, scientific name, larval stage, aphia ID, urn #
if (is_empty(taxa_file)) {
    
    # extract taxa names, separating names and life stages
    # taxa.name <-
        taxa  %>% #%$% as.vector(Taxa)
         select(taxa_orig = taxa) %>%
        mutate(
            taxa = str_remove(
                taxa_orig, 
                regex(
                    "sp{0,2}\\.|\\(unknown\\)|\\(.*\\)", 
                    ignore_case = TRUE
                    )
            ),
            lifeStage = str_extract_all(
                taxa,
                regex(
                    regex_lifestage,
                    ignore_case = TRUE
                ),
                simplify = TRUE
            ) %>%
                str_to_lower(),
            taxa = str_remove(
                taxa,
                regex(
                    regex_lifestage,
                    ignore_case = TRUE
                )
            )
        ) 
    
    # ---- run taxa matching with WoRMS database ----
    taxalist <- match_taxa_fix(taxa.name$taxa, fuzzy = TRUE, ask = T)
    
    taxa_matched <- bind_cols(taxa.name, taxalist)
    
    # ---- save first WoRMS database search ----
    write_csv(taxa_matched,
              paste0(
                  here::here("data", "metadata", "aphia_taxa"), 
                  format(Sys.time(), '_%Y%m%d_%H%M%S'),
                  ".csv"),
              na = ""
              )
    
    
} else {
    # ---- read taxa file ----
    taxa_matched <-  readr::read_csv(taxa_file, show_col_types = FALSE) 
}

# ---- fix non-matched taxa names ----
# if some taxa have NA names, can try again on this subset
taxa_ntmtch <- taxa_matched %>%
    filter(is.na(scientificName)) %>%
    select(1:3)

if (nrow(taxa_ntmtch) > 0) {
     # ---- run taxa matching with WoRMS database ----
    taxa_new <- match_taxa_fix(taxa_ntmtch$taxa, fuzzy = TRUE, ask = T)
    
    taxa_mtchnew <- bind_cols(taxa_ntmtch, taxa_new)
    taxa_matched <-
        bind_rows(taxa_matched, taxa_mtchnew)
    
    # ---- save first WoRMS database search ----
    write_csv(taxa_matched,
              paste0(
                  here::here("data", "metadata", "aphia_taxa"), 
                  format(Sys.time(), '_%Y%m%d_%H%M%S'),
                  ".csv"),
              na = ""
              )
}
```

```{r match-data-taxa}
# ---- fix new taxa names ----
# TODO: read in all files, joining with scientific names, then add more if they 
# don't exist
taxa_matched_merg <- left_join(taxa, taxa_matched, by = c("taxa" = "taxa_orig")) 

taxa_to_match_merg <- taxa_matched_merg %>%
    filter(is.na(taxa.y)) %>%
    select(taxa_orig = taxa, taxa = taxa.y, lifeStage)

if (nrow(taxa_to_match_merg) > 0) {
     # ---- run taxa matching with WoRMS database ----
    taxa_new <- match_taxa_fix(taxa_to_match_merg$taxa, fuzzy = TRUE, ask = T)
    
    taxa_matched <- bind_cols(taxa_to_match_merg, taxa_new) %>%
        bind_rows(taxa_matched, .)
    
    # ---- save first WoRMS database search ----
    write_csv(taxa_matched,
              paste0(
                  here::here("data", "metadata", "aphia_taxa"), 
                  format(Sys.time(), '_%Y%m%d_%H%M%S'),
                  ".csv"),
              na = "")
}

taxa_matched_merg <- left_join(taxa, taxa_matched, by = c("taxa" = "taxa_orig")) 

file.name <-
    glue(
        "{here::here('data', 'processed')}/
        \b{taxa_matched_merg$cruise_id[1]}_
        \b{taxa_matched_merg$site[1]}_
        \b{taxa_matched_merg$mesh[1]}um_
        \btaxa_merged
        \b{format(Sys.time(), '_%Y%m%d_%H%M%S')}
        \b.csv"
    ) %>%
    str_remove_all("(\\\n\\\b)")

write_csv(
    x = taxa_matched_merg,
    file = file.name,
    na = "")

```

# tried a different database without working
```{r}
# library("taxize")
# 
# 
# 
# cbind(taxa.name, test)
# 
# ?get_tsn
# get_tsn(taxa.name[1:5])
# get_tsn(("Acartia"))
# get_wormsid(test)
# get_wormsid("fish", searchtype = 'common')
# 
# rgbif::name_backbone("Acartia")
# 
# obistools::match_taxa(c("Acartia","Candacia","copepod","Euterpina acuntifrons"), ask = T)
```

```{r}


# obis.taxa <- obistools::match_taxa(taxa.name, ask = T)
# 
# robis::taxon(11676)
# 
# # with this 20 have NA without ability to ask for input
# obis.taxa %>%
#     mutate(taxa.name, .before = 1)
```

