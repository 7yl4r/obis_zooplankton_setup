---
title: "Zooplankton Data to OBIS Format"
author: "Sebastian DiGeronimo"
date: '2022-07-08'
output: html_document
---

```{r}
root <- rprojroot::find_rstudio_root_file()
knitr::opts_chunk$set(echo = TRUE)
library("readxl")
library("hablar") # might be useful when needing to fix names
library("worrms")

library("ggplot2")
library("tibble")
library("tidyr")
library("readr")
library("purrr")
library("dplyr")
library("stringr")
library("forcats")
library("lubridate")
library("glue")
library("fs")
library("magrittr")
library("broom") # optional

library("tidyverse")
library("ggforce")
library("geosphere")
library("vroom")
library("plotly")
```

Workflow:
https://github.com/ioos/bio_data_guide/tree/main/OBIS_data_tiers
Needed: 3 things
    1. Event
    2. Occurances
    3. Measurement of Fact (MOF)



1. Read and format metadata spreadsheets
    - convert to DarwinCore format 
    
```{r}
root <- rprojroot::find_rstudio_root_file()
files <- fs::dir_ls(paste0(root,"/data/metadata/"), recurse = T,
           regexp = "^[^~]*\\.(xlsx|csv)$") 
files <- files[!grepl("BB3|Digna|Schedule|Kelble", files)][-1]

for (i in seq(files)) {
# for (i in 1) {
    print(basename(files[i]))
    temp_sht <- readxl::excel_sheets(files[i])
    
    # ---- detect if sheets contain zooplankton ----
    # reduces the amount of opening sheets of a file if one is named zooplankton
    # if not, will try all of them
    dect <- any(str_detect(temp_sht, "zooplankton"))
    
    if (isTRUE(dect)) {
        temp_sht <- "zooplankton"
    }
    
    rm(dect)
    
    # ---- open file and read sheet ----
    for (j in seq(temp_sht)) {
        skips <-
            tryCatch({
                which(
                    readxl::read_xlsx(
                        files[i],
                        range = cell_cols("A"),
                        col_names = FALSE,
                        sheet = temp_sht[j]
                    ) == "ZOOPLANKTON SAMPLING"
                )
            }, warning = function(w) {
                message(sprintf("Warning in %s: %s", deparse(w[["call"]]), w[["message"]]))
                
            }, error = function(e) {
                message(sprintf("Error in %s: %s", deparse(e[["call"]]), e[["message"]]))
                integer()
            }, finally = {
                
            })
        
        
        if (!is_empty(skips)) {
            temp <-
                readxl::read_xlsx(
                    files[i],
                    sheet = temp_sht[j],
                    skip = skips,
                    .name_repair = janitor::make_clean_names,
                    na = c("skip", "Did not collect", "not recorded")
                ) %>%
                janitor::remove_empty(which = "rows") %>%
                tidyr::fill(station) %>%
                drop_na(flowmeter_out) %>%
                tidyr::fill(contains(c("lat", "lon", "date", "time"))) %>%
                mutate(
                    try(across(contains(c("time_gmt")),
                           list(time_gmt = ~ hms::as_hms(
                               format(.x, format = "%H:%M:%S",
                                        tz = "utc")
                           )),
                           .names = "{.fn}"), silent = T),
                    try(across(contains(c("time_gmt")),
                           list(date_time = ~ ymd_hms(paste(date, .x),tz = "utc")),

                           .names = "{.fn}"), silent = T),
                    .after = 4
                )

            if (is.character(temp$lat_in) | is.character(temp$lon_in)) {
                temp %<>%
                    separate(lat_in, into = c("latdeg", "latmin"), " ") %>%
                    separate(lon_in, into = c("londeg", "lonmin"), " ") %>%
                    mutate(
                        across(latdeg:lonmin, as.numeric),
                        lat_in = latdeg + (latmin / 60),
                        lon_in = -(londeg + (lonmin / 60)),
                        .before = latdeg
                    ) %>%
                    select(-(latdeg:lonmin))
            }

            temp <- tryCatch({
                hablar::retype(temp,-time_gmt)
            }, error = function(e) {
                message(sprintf("Error in %s: %s", deparse(e[["call"]]), e[["message"]]))
                temp
            })

            
            if (nrow(temp) == 0) temp <- NULL
            
            do.call("<-", list(tools::file_path_sans_ext(basename(files[i])), temp))
            message("SUCESSS")
            next
        } else {
            message(paste0(
                "Might not be this one:\n",
                tools::file_path_sans_ext(basename(files[i]))
            ))
        }
        
    }
}


rm(i, j, skips, temp_sht)
rm(list = names(which(sapply(
    globalenv(), is.null)))) # or .GlobalEnv

```

Files without info:
October 2017
March 2018
April 2018
April 2021
April 2019

```{r}
meta_list <- ls()[str_detect(ls(),"fknms")]



meta_full <- mget(meta_list) %>%
     discard(function(x) is.null(x)) 

# see where issue will lie when joining
janitor::compare_df_cols(meta_full,
                         return = "match")

# test1 <- 
     # reduce(test[c(1:6,8:12)], full_join )

meta_full$`fknms_logsheet_July 2016_Hepner` <- meta_full$`fknms_logsheet_July 2016_Hepner` %>%
    mutate(date_time = NA) %>%
    rename(time_gmt = local_time_est)


meta_full <- meta_full %>%
    map(.x, .f=~select(.x, 1:15))

meta_df <-
    do.call(rbind.data.frame, meta_full)   %>%
    rownames_to_column("file")  %>%
    mutate(
    file = str_remove(file,pattern = "\\.\\d{1,3}$"),
    mesh_size_um = strsplit(as.character(mesh_size_um), "/"),
    ship_speed_knots = str_remove_all(ship_speed_knots, "~"),
    # date_time = format_ISO8601(date_time),
    ) %>%
      unnest(mesh_size_um) %>%
    mutate(
        mesh_size_um = str_trim(mesh_size_um)
    )

rm(list = meta_list)
```

MOF = measurement of fact

Event                          - where/when took place

Occurrence                     - species info, presence/absence
Occurrence MOF                 - quantity, other info to measure them

MOF                            - environmental quantities measured 

# not finished yet
occurance        = 
        institution_code = "USF_IMaRS"
        collection_code  = "compiled_zoo_taxonomy_nlf"
        catalog_number   = "20yy_mm_dd"
        occurrenceID     = (unique identifier)
                        joining of 
                            * urn:catalog, 
                            * institution code
                            * collection_code
                            * catalog_number
                            * row number 
                            * join by :
        eventDate        = date_time
        decimalLongitude = lon_in
        decimalLatitude  = lat_in
        scientificName   = natalia spreadsheet
        scientificNameID = "TODO: worms lookup", #taxonomy_df["classification"],

occurance =
        basisOfRecord    = "HumanObservation",
        collectionCode   = collection_code,
        catalogNumber    = catalog_number,
        occurrenceStatus = "present",
        institutionCode  = institution_code

extra-stuff
        mesh_size 
        flowmeter in
        flowmeter out
        ship speed
        inpeller constant
        distnace
        tow speed
        formalin
        vol filtered


Occurrence Bare Minimum:
```{r}
data.table::data.table(
      occurrenceID = c("my-dataset-0001c29"),
   decimalLatitude = c(-87.7575),
  decimalLongitude = c(24.4727),
    scientificName = c("Sparisoma aurofrenatum"),
  scientificNameID = c("urn:lsid:ipni.org:names:37829-1:1.3"),
  occurrenceStatus = c("present"),
     basisOfRecord = c("HumanObservation"),
         datasetID = c("my-dataset-tylar-2020-01-08-123456"),
         eventDate = c("2010-01-03T13:44Z")
)


meta_df %>%
    select(split_size) %>%
    mutate(
        split_zie = eval(parse(split_size))
    )
map(meta_df$split_size, function(x) {
    out <- tryCatch({
        eval(parse(text = x))
    }, error = function(e) {
        message(sprintf("Error in %s: %s", deparse(e[["call"]]), e[["message"]]))
        NA_integer_
    })
    return(out)
})
  
# record level
rcd_lvl <- 
    meta_df %>%
        dplyr::transmute(
            type                = "Event",
            modified            = Sys.Date(),
            language            = "en",
            license             = "http://creativecommons.org/publicdomain/zero/1.0/legalcode",
            institutionCode     = "USF_IMaRS",
            datsetID            = "FKNMS_MBON_", # which one?
            datasetName         = "MBON Florida Keys National Marine Sanctuary", 
            basisOfRecord       = "HumanObservation", 
            informationWithheld = "collector & analyst identities withheld",
            )

# event 
event <-
meta_df %>%
dplyr::transmute(
    catalogNumber   = row_number(),
    locationID      = case_when(
        str_detect(station, "Mol") ~ "MR",
        str_detect(station, "Loo") ~ "LK",
        str_detect(station, "West") ~ "WS",
        TRUE ~ station
    ),
    parentEventID      = "MBON_zooplankton", 
    datasetID          = glue("{parentEventID}:cruiseID"), # will be {cruise_id}
    eventDateTime      = format(date_time, "%Y-%m-%dT%H:%m:%S%z"),
    # eventID - something like cruiseID:stationID:meshsize
    # {{mbon} : cruise} : stn : mesh : date_time
    eventID            = glue("{datasetID}:stn_{locationID}:{mesh_size_um}_um:{eventDateTime}")  %>%
    str_remove("\\+0000"), # will be {cruise_id}
    fieldNumber        = glue("cruiseID {station} {mesh_size_um}"),
   
    eventDate          = date(date_time),
    eventTime          = format(date_time,"%H:%m:%S%z"),
    year               = year(date_time),
    month              = month(date_time),
    day                = day(date_time),
    # samplingProtocol = paste(mesh_size_um, "mesh size (um)"),
    samplingProtocol   = glue("{mesh_size_um} mesh size (um) - bongo nets| http://drs.nio.org/drs/handle/2264/95"),
    # TODO: habitat NERC vocab
    habitat            = "near reef",
    sampleSizeValue    = "something like distance traveled? in meters?", # volume filtered
    sampleSizeUnit     = "would equal metre?" # volume
)

# location
location <-
    meta_df %>%
    dplyr::transmute(
        eventID           = event$eventID,
        decimalLatitude   = lat_in,
        decimalLongitude  = lon_in,
        higherGeographyID = case_when(
            str_detect(station, "Mol|Looe|West") ~ "http://vocab.getty.edu/tgn/7030258",
            str_detect(station, "5") ~ "http://vocab.getty.edu/tgn/1101513",
            TRUE ~ "Not Found"
        ),
        higherGeography   = "North America | United States | Florida",
        continent         = "North America",
        country           = "United States",
        countryCode       = "US",
        stateProvince     = "Florida",
        geodeticDatum     = "EPSG:4326",
        georeferencedBy   = "NOAA AOML | USF IMaRS | RSMAS R/V Walton Smith"
    )


# occurrence
# occur <- 
data %>%
   dplyr::transmute(
       eventID                  = event$eventID,
       occurrenceID             = glue("{eventID}:{aphiaID}:{lifeStage}"),
       recordedBy               = "NAt?", # check these
       recordedByID             = "may include orcid?", # check these
       
       # IDk which of these is accurate
       individualCount          = `# Ind/Sample`,
       organismQuantity         = `Ind/m^3`, # or `# Ind/Sample` and leave out individualCount?
       organismQuantityType     = "Individuals per cubic metre", # or
       measurementType          = "Number per cubic metre", # MOF? should also include ind/sample?
       measurementUnitID        = "http://vocab.nerc.ac.uk/collection/P06/current/UPMM/", # MOF?
       
       # --- 
       lifeStage                = `lifeStage`, # need to infrom natalia this can be included
       establishmentMeans       = "native | uncertain",
       occurrenceStatus         = "present", # need to filter ind/sample > 0
       preparations             = "formalin | ethanol",
       scientificName           = taxa,
       scientificNameID         = taxaID, # like urn:lsid:ipni.org:names:37829-1:1.3
       basisOfRecord            = "HumanObservation", 
       datasetID                = "x",
       eventDate                = "date",
       identificationReferences = "WoRMS",
       verbatimIdentification   = taxa_orig
       ) 

# MoF https://github.com/ioos/bio_data_guide/blob/main/datasets/WBTS_MBON/IOOS%20DMAC%20DataToDwC_Notebook_event.ipynb
tibble::tribble(
              ~Origin.Term,                                       ~`measurementTypeID`,       ~URI,
                "Net_Type",                                             "plankton net",       "22",
               "Mesh_Size",                                   "Sampling net mesh size", "Q0100015",
               "NET_DEPTH",       "Depth (spatial coordinate) of sampling event start", "DXPHPRST",
                 "COMMENT",            "N/A (mapped to measurementRemark field above)",      "N/A",
       "Plankton_Net_Area",                    "Sampling device aperture surface area", "Q0100017",
         "Volume_Filtered",                                                   "Volume",      "VOL",
            "Sample_Split", "N/A (information added to measurementRemark field above)",      "N/A",
       # "Sample_Dry_Weight",                                       "Dry weight biomass", "ODRYBM01",
                # "DW_G_M_2",                                       "Dry weight biomass", "ODRYBM01",
         "Dilution_Factor",                                                      "???",      "???",
    "TOTAL_DILFACTOR_CFIN",                                                      "???",      "???"
    )
 
column_mappings <- 
    tibble::tribble(
                        ~types,                    ~uri,     ~unit,             ~unitID, ~accuracy,                        ~type,         ~measurementMethod,
                    "Net_Type",       "L05/current/22/",        NA,                  NA,        NA,                   "net type",                         NA,
                   "Mesh_Size", "Q01/current/Q0100015/", "microns", "P06/current/UMIC/",        NA,                  "mesh size",                         NA,
                   "NET_DEPTH", "P01/current/DXPHPRST/",       "m", "P06/current/UPAA/",        NA,                  "net depth",                         NA,
           "Plankton_Net_Area", "Q01/current/Q0100017/",      "m2", "P06/current/UPAA/",        NA,          "plankton net area",                         NA,
             "Volume_Filtered",      "P25/current/VOL/",      "m3", "P06/current/UPAA/",        NA,            "volume filtered", "geometrically determined",
             "Dilution_Factor",                      NA,      "ml", "P06/current/VVML/",        NA,            "dilution factor",                         NA,
        "TOTAL_DILFACTOR_CFIN",                      NA,      "ml", "P06/current/VVML/",        NA, "Total Dilution factor CFIN",                         NA,
                "Sample_Split",                      NA, "decimal", "P06/current/UPCT/",        NA,               "sample split",          "Folsom Splitter"
        )

clipr::write_clip(calc)
tibble::tribble(
    ~date_collected, ~sample_id, ~filtered_volume_m3, ~pipette_vol_m_l, ~dillution, ~dillution_factor, ~mesh, ~date_analyzed, ~split_amount,
       "2015-11-16",  "MR 1115",                 97L,               5L,       250L,               50L,  500L,   "2021-07-23",            2L
    )

# http://vocab.nerc.ac.uk/collection/L05/current/22/
# might need to pivot wider first, then pivot longer



mof <- 
    occur %>%
    dplyr::transmute(
        occurrenceID      = occur$occurrenceID,
        eventID           = event$eventID,
        measurementValue  = `data$ind/m^3`,
        measurementType   = "Number per cubic metre", # MOF?
        measurementID     = "http://vocab.nerc.ac.uk/collection/S06/current/S0600002/",
        measurementUnitID = "http://vocab.nerc.ac.uk/collection/P06/current/UPMM/", # MOF?
    )


```

